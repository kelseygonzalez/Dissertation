\hypertarget{paper-1}{%
\chapter{Digital Trace Data as Indicators of the Social World: Validating Google Trends for use in Scientific Research}\label{paper-1}}


\section{Abstract}

Computational social science and other disciplines are quick to 
adopt new sources of digital trace data for use in academic research. 
In this article, I examine the validity of Google Search Trends 
as potential indicator of three different cases, namely attitudes, 
disease prevalence and political preferences using eight %TODO double check number
different validated data sources. I use Pearson and Repeated Measures Correlations 
between the Google Trends and the validated indicators as well as multiple linear regression
(for cross-sectional datasets) and random intercept hierarchical linear models
(for longitudinal datasets) as an additional test of the data. 
I fail to find moderate or strong correlation among any of the Google 
Trends tested and their validated indicators. While some Google Trends 
tested were significantly associated with the outcome in the regression models,
effects tended to be small and the total model explainability remained low,
even when controlling for demographic variables. This article shows that there is no 
no external validity of Google Trends for these uses and social scientists
will find no replacement for high quality survey data with Google Trends. 

\textbf{Keywords}: Social Science Methodology, Digital Trace Data, Information-Seeking Behavior, Google Trends, Information and communications technology, Validity

\section{Introduction}
New big data sources have led to vast possibilities for social science
research because they are bigger, cheaper, and already available
\citep{kingEnsuringDataRichFuture2011,lazerComputationalSocialScience2009,salganikBitBitSocial2017}.
Before overenthusiastically embracing these
sources into our workflows, social scientists must clearly establish
parameters under which these data sources should be operationalized
\citep{bailCulturalEnvironmentMeasuring2014, lazerParableGoogleFlu2014}. As
prior research outlined, the "quantity of data does not mean that one
can ignore foundational issues of measurement and construct validity and
reliability and dependencies among data" 
\citep[p. 1203]{lazerParableGoogleFlu2014}. 

With the expansion of big data, some computational social science research has
developed extremely innovative methods that lead to groundbreaking results using 
big data that are shown to be reliable. As an example,
\citet{blumenstockPredictingPovertyWealth2015} use
county-level cell-phone records to construct the distribution of poverty
and wealth in Rwanda, a country where national surveys and censuses are
rare and costly. However, \citet{blumenstockPredictingPovertyWealth2015} go to
great lengths to demonstrate that their operationalization of the cell
phone data creates a reliable and valid construct; few social science
papers utilizing big data dig into the construct validity of their
metrics to this extent and even fewer publications focus on
methodological guidelines of how to use sources of big data
\citep[For exceptions, see ][]{asseoTrackingCOVID19Using2020, stilesAssessingCriterionValidity2018}.
However, research has shown the small adjustments to an algorithm or
metric may void any research insight we are able to pull from such data
\citep{lazerParableGoogleFlu2014}.

\subsection{Google Trends}

One source of big data that has yet to be widely utilized are Google Search Trends. 
Google Trends is an open data resource provided by Google, a major internet search
engine worldwide, that analyzes the top search queries across different localities, 
languages, and time. While Google Trends is freely available and holds a breadth of 
online activities that could potentially be of use in social science, few social 
scientists outside of economics \citep[see][for examples]{choi2012predicting, jun2018ten,da2011search} have used this resource.
Because of this, I set out to establish how can we operationalize Google Search Trends as
validated indicator for uses in social science research to provide guidance to social 
scientists as they adapt different sources of big data into their toolboxes. 

Building on this prior research outlining various issues with different sources of big data
\citep{boydCriticalQuestionsBig2012,lazerIssuesConstructValidity2015},
this paper tests the construct validity of Google Search Trends as an indicator of three different
cases, namely attitudes, disease prevalence and political preferences.
These three cases will be tested using attitudinal indicators from the %NORC General Social Survey, 
CDC \citeyearpar{vaches_data} and New York Times \citeyearpar{mask_data}, 
health indicators from the CDC \citeyearpar{suic_data}, 
US cases rates of Covid-19 from The New York Times \citeyearpar{covid_data},  
and political indicators from the historical US Presidential Election results \citeyearpar{pres_data}, 
and the American National Election Survey \citeyearpar{anes_data}. This paper will contribute to the
creation of methodological norms and standards of how to use Google
Trends as a big data source for societal research and serve as a
critical inquiry into the adoption of big data without a critical eye
for the ecological validity of the sources.

\subsection{Google Trends and research on attitudes}

I will use three categorizations of ways I propose Google Trends could
be operationalized for social scientific usage. First, I'll test Google
Trends as an operationalization of attitudinal indicators. 
After \citet{bailCulturalEnvironmentMeasuring2014}'s call for
cultural sociologists to utilize the ever-expanding world of big data,
Google trends began appearing as a data source in sociological and
social science research. From research on mass shootings and firearms \citep{brownsteinInternetSearchPatterns2020, semenzaInformationseekingWakeTragedy2020}, 
protest and anti-Muslim sentiment \citep{bailUsingInternetSearch2018,barrieSearchingRacismGeorge2020,grossThereFergusonEffect2017},
to analyzing country-level changes in social perception \citep{reyes_etal18},
Google search trends are a new and innovative indicator of cultural interest.
Extending into social networks and culture, \citet{bailPrestigeProximityPrejudice2019}
even used Google trends to measure how 'culture' spreads around the globe.

\subsection{Google Trends and research on health behaviors}

Google Search Trends have been used continuously in estimations of
disease prevalence and population health in journals like the Journal of
Medical Internet Research. While much of this research has focused on
the Covid-19 pandemic \citep{jimenez_etal20, jimenezCOVID19SymptomGoogle2020, limEstimatingInformationSeekingBehaviour2020, mavraganiCOVID19PredictabilityUnited2020, nguyenGoogleTrendsAnalysis2020, todorovaInternetBasedData2021, mingUnderstandingHealthCommunication2021},
other research has investigated Google Trends as indicators of wellbeing
\citep{brodeurCOVID19LockdownsWellbeing2021, carpiTwitterSubjectiveWellBeing2020, duCOVID19IncreasesOnline2020},
suicidality \citep{burnettTimeTrendsPublic2020}, 
vaccination uptake \citep{dalumhansenEnsembleLearnedVaccination2016}, 
obesity \citep{sarigulNowcastingObesityUsing2014}, 
and even insomnia \citep{zittingGoogleTrendsReveal2020},
to cover a few examples. For a partial review of other utilizations, see \citet{nutiUseGoogleTrends2014}. According to
\citet{jaidkaInformationseekingVsSharing2021}, the majority of studies profess
a correlation of \> .70, "demonstrating the vast potential of Google
Search as a proxy for monitoring population health" (p. 3) based on
assumptions that individuals search because of self-diagnosis and to
identify possible courses of treatment \citep{dechoudhurySeekingSharingHealth2014}.


\subsection{Google Trends and research on politics}

Various sources have also used Google Trends as a way to forecast
political elections and political attitudes
\citep{wolfTrendingRightDirection2018}. For instance,
\citet{swearingenGoogleInsightsSenate2014} investigate how U.S. Senate
Elections relate to attention measured by search traffic.
\citet{prado-romanGoogleTrendsPredictor2020} compare how Google Search trends
are able to predict presidential election results in both the United
States and Canada. Finally, the OECD Development Centre is investigating
how Google data can help elucidate governments' approval in Latin
America \citep{montoyaUsingGoogleData2020}. 
% TODO what are the results of these papers? do they show promise? 

\subsection{Causes for concern}

While Google Trends has not been widely used or tested in social science,
I am not the first author to investigate its validity.
In contrast to the papers mentioned above, there are researchers who have cautioned against
the wide usage of Google Search Trends. For instance, \citet{asseoTrackingCOVID19Using2020} 
concludes in their research on Covid-19 case rates and Google searches that the correlation 
is often spurious and due to confounding. They even explain that "searches would
result not only from self-symptoms, but also from interest elicited by
media coverage" \citep[][p.1]{asseoTrackingCOVID19Using2020}.

\citet{rovetta21} also investigate the reliability of Covid-19 searches to investigate the quality of the data and find that there are various issues with missing data in smaller regions and significant variation in correlations between regions throughout the study period, which they indicated was a cause for concern more than a product of behavioral changes. 

\citet{cebrian_domenech22} emphasize in their research that there are some 
non-negligible issues with the measurement of Google Trends, pointing to 
errors within the purview of accuracy, completeness, consistency, and validity \citep{KarrDataQuality}. \citet{cebrian_domenech22} point not only to 
meta issues like the lack of transparency into the sampling methods 
and population coverage, but they identify real differences 
in the data provided by the Google Trends API for the same query, pointing
to likely internal process used by Google to sample their data to compute the trends.
These authors show that the variability of the results over time for the same query
can alter the results of analysis and forecasting significantly; they propose
caution in the use of the digital trace data for other researchers until
further research can determine the cause for the inaccuracies. 

Finally, it is important to note that a majority of research using 
Google Trends as digital trace data do so to indicate
populations \textit{attention} to a search query. Because
searches can be done with many different intentions, such as 
information gathering, ridicule, navigational purposes, it may
be hard to declare that an overall level of engagement with a topic
on Google Search may be indicative of anything more than interest \citep{da2011search}. 
\citet{jungherr_etal17} explore this topic deeply with data from
Twitter on German federal election candidates and find that there is no
indication that tweets about candidates are indicative of or can predict 
political support. Instead, they conclude that tweets merely indicate the
temporal dynamics of public attention toward politics. 

Building on this previous research in the reliability and accuracy of the trends, 
it is imperative that, as a discipline, we are able to provide a framework for
how to use different sources of digital trace data. This aim of this paper is
to take the first step to build that framework. I will attempt to validate 
Google Search Trends for use outside of mere interest or attention using the
methods that follow; specifically, to verify whether
trends can be used to indicate attitudes, health outcomes, or political support. 

\section{Methods}
To investigate the construct and criterion validity of the use of Google Trends
in these three areas, I gathered geo-located social science data across multiple
sources to address the three areas of inquiry for this paper.  Table
\ref{tab:data-sources-table} outlines which data sources are used for this
project and which trends are matched to each source.

\input{tables/paper1/data-sources-table}

\subsection{Measures}

\subsubsection{Google Trends}
This paper focuses on a validation of Google search trends \citep{googletrends}.
Google trends portray the search frequency for specific search terms across
designated media markets areas (DMAs), a nonoverlapping aggregation of U.S.
counties to 210 media markets based on similar population clusters \citep{dma_key}.
Raw data is on a scale from 0 to 100, with 100 being the maximum search
popularity out of all DMAs. When available, I use Google search topics to
measure trends as opposed to search terms. Search topics are a more robust
measurement than a single search term: topics are aggregations of the rates of
multiple, highly correlated search terms together into a cohesive topic. For
example, while 'Beyoncé', 'Beyonce' and 'beyonce knowles' are all separate
search terms, 'Beyoncé Knowles' encompasses all of these into a single search
topic. Google Search Trend are only available cross-sectionally (a single time
period across a geography) or as time-series (a single geo-location across
time). To remedy this and build a longitudinal dataset of each search topic for
the longitudinal datasets, I follow the method proposed in \citet[p. 5]{park_etal}.
This method involves building a dataset of unscaled cross-sectional values,
selecting a DMA to use to establish the rescaling ratio (I use 'Los Angeles
CA'), and then finding the time-series values for the one DMA. To find the
rescaling ratio for each week in the time-series, you divide the time-series
value for each week by the cross-sectional value for each week, resulting in a
rescaling vector to be used for all weeks in the dataset across geographies. To
rescale each longitudinal value, multiply the respective week's rescaling ratio
by the cross-sectional value. Rescaled longitudinal data was compared against
time-series data for multiple test counties and was equivalent. For a more
in-depth explanation of this procedure, see \citet[p. 5]{park_etal}. Missing
datapoints in longitudinal datasets were filled in with interpolated values
using \texttt{zoo\:\:na.approx()} \citep{zoo}.

\subsubsection{Attitudinal}

% <!-- \citep{gss_data} (let's see if I get access first) -->

One measure of attitudinal indicators is the Vaccine Hesitancy for COVID-19
\citep{vaches_data}. The CDC uses the U.S. Census Bureau’s Household Pulse Survey
(HPS) and the 2019 American Community Survey (ACS) 1-year Public Use Microdata
Sample (PUMS) to measure U.S. residents’ intentions to receive the COVID-19
vaccine if available during May 26, 2021 – June 7, 2021. This dataset consists
of 3,148 observations, one for each U.S. County. The variable
measures the percent of adults in the county who describe themselves as
“unsure”, “probably not”, or “definitely not” going to get a COVID-19 vaccine
once one is available to them. The variable ranges from 4.99\% to 32.33\%. 

Another attitudinal indicator I use is the Mask-Wearing Survey Data conducted by
Dynata for the New York Times from July 2 through 14, 2020 \citep{mask_data}. 250,000
survey respondents were asked, "How often do you wear a mask in public when you
expect to be within six feet of another person?". The NYT weighted each response
to create a county level measure of what percent of the county never, rarely,
sometimes, frequently, and always wore a mask when in public. This dataset
consists of 3,148 observations, one for each U.S. County included in the sample. This measure
represents the percent of adults in the county who never or rarely wear a mask
(range = 0.10\% to 55.80\%).

\subsubsection{Health}

I also use U.S. Covid-19 rates to validate health and disease related topics. I
retrieve U.S. county-level Covid-19 rates from by \citet{covid_data}, who compile this
data based on reports from state and local health agencies. It is widely
acknowledged that there are biases in this data due to inconsistencies and
availability in testing as well as different community propensity to test
\citep{gu22, cdc20a} However, it is the best measure we have of actual case rates.
Case Rate is measured as number of cases per 100,000 population. Observations
vary from a minimum of 0 to a maximum of 1460.46 for each Monday from January 27, 2020 through
December 27, 2021. There are 285,986 cases across 3,136 counties and 101
dates. Missing data were interpolated using `zoo::na.approx()` \citep{zoo}.

I also use county-level suicide rates from the US Centers for Disease Control
and Prevention \citeyearpar{suic_data}. Data is grouped by year from 2010-2020. Raw death
rates are scaled by population size for each year and can be interpreted as the
death rate by suicide for every 1000 people. There are 34,683 total
cases, resulting from 34,617 observations of 3,147 counties. Missing data were interpolated using
zoo\:\:na\.approx() \citep{zoo}. Measures range from 0.034 to 53.254.


\subsubsection{Political}

Finally, I test Google Trends as an operationalization of political
attitudes by first looking at actual voting outcomes in historical US
Presidential Election results. This data comes from  \citet{pres_data}, who scraped the
results from Townhall.com, Fox News, Politico, and the New York Times. Data on 
presidential outcomes were available for 3,147 counties in 2016 and
3,118 counties in 2020. Each variable measures the percent of votes
for the candidate, with the lowest percent at 3.09\% and the highest at 92.15\%.

In addition, I use data on political opinions from the the American National
Election Survey 2020 Time-Series Study \citep{anes_data}. This data is paired with
restricted data provided by ICPSR, the Inter-university Consortium for Political
and Social Research, that contains the geo-ids of the 5,441
survey respondents. The study interviewed respondents in a pre-election survey
that was conducted between August 18, 2020 and the day of the US Presidential
election day, November 3, 2020.

% TODO: WHICH VARIABLES
% TODO: VARIABLE SCALE

\subsubsection{Other}

In addition to specific outcomes of interest, I also gathered various county
controls to investigate possible variable confounding. The first Seven of these
variables come from the 2010-2019 5-year American Community Surveys
\citep{acs2019, acs2018, acs2017, acs2016, acs2015, acs2014, acs2013, acs2012, acs2011, acs2010}. 
These data include total population, population density,
unemployment rate for those 16 and over, median county income, average commute
time, percent of households living under the poverty line, and percent above 65
years old. In addition, some models employ the U.S. Current Population Survey \&
American Community Survey Geographic Estimates of Internet Use, 1997-2018
\citep{internet_use} to estimate households with broadband internet subscriptions.
This final variable is an attempt to capture the latent propensity to use the
internet for information search.

% ACS 5 year estimates
% a00001 total population
% a00002 population density
% a17005 unemployment rate 16+
% a14006 median income
% a09003 average commute time
% poverty percent
% 01001B 65 plus

\subsection{Analysis}
Google Search Trends data and additional demographic data were merged with each
individual indicator based on County FIPS Codes and date using \texttt{dplyr} in \texttt{r}, version 4.1.2 \citep{tidyverse}. After creating these
different datasets, I use the Pearson correlation formula (formula \eqref{eq:pearsoncorr}) for cross-sectional numeric data to calculate the
strength of the relationship between each Google Trend and the respective
data source.

\begin{equation}
 r =
  \frac{ \sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y}) }{
        \sqrt{\sum_{i=1}^{n}(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^{n}(y_i-\bar{y})^2}} \label{eq:pearsoncorr}
\end{equation}

To address longitudinal correlations, I employ Repeated Measures Correlation
using the `rmcorr` package in \texttt{r} \citep{bland1995, bakdash2017}. 
Repeated Measures Correlation is useful for determining the within-county
association for paired measures across time and counties. Results of these 
correlations can be seen in table \ref{tab:corr-results}.

As an additional test of the relationships, I employ multiple linear regression
(for cross-sectional datasets) and random intercept hierarchical linear models
\citep{pinheiro_etal21} (for longitudinal datasets) to identify the strength of
relationships across locales. I include city-data like county population size,
broadband rates, and median income to attempt to disentangle possible
confounders of the relationship between Google search Trends and verified
indicators. Identifying these possible confounded relationships will help to
explain why some articles find relationships between the trends and outcomes
while others did not. For linear regression models, I normalize independent
variables, i.e.variables have been centered and scaled to have a mean of 0 and
standard deviation 1. Results of these models can be seen in tables \ref{tab:vacc_hes_analysis} through \ref{tab:pres_2020_analysis}.

\section{Results}

\subsection{Validating Google Trends as metrics of attitudinal indicators}

% \citep{gss_data} (let's see if I get access first)

\input{tables/paper1/corr-results}

The first attitudinal indicators is Vaccine Hesitancy for COVID-19 vaccines
\citep{vaches_data}. When comparing the measure of vaccine hesitancy to four
different Google Search Trends, the correlation does not exceed -0.4657
('Coronavirus (Disease)' and vaccine hesitancy). Correlation's under |0.40| are
considered to be weak according to the common rules of thumb. When running these
correlations in multiple linear regression (see the results in table
\ref{tab:vacc_hes_analysis}), I see an $r^2$ of 0.233 (Model 1) and 0.411
(Model 2), indicating that the Google Trends are able to explain about 23% of
the variation in Vaccine Hesitancy alone. Demographic characteristics like the
percentage of households with broadband internet and the population density are
able to explain about 35\% of the variation (model 2), outperforming the first
models. The Trend coefficients themselves, however, are significant and remain
significant when controlling for demographics. This reinforces the finding from
the Pearson Correlation that there is a significant but weak relationship
between the Google Trends and Vaccine Hesitancy.

\input{tables/paper1/vacc_hes_analysis}

The second attitudinal measure I test is how Google Trends relates to rare mask
usage. As with vaccine hesitancy, the Pearson correlations are negligible.
Correlation's under |0.20| are considered to be negigible according to the
common rules of thumb. I introduce these trends in multiple linear regression in
table \ref{tab:mask_analysis}. Model 1 demonstrates that these five Google
Search Trends can explain about 7\% of the variance in mask usage across U.S.
counties, reinforcing the conclusion that the relationship is quite weak. The
coefficients themselves are somewhat significant in Model 1. However, after
including the demographic variables in Models 2 and 3, we see that the
relationship between the Google Search Trends and mask usage is strengthened in
magnitude and in significance, likely indicating a suppression effect due to
underlying relationships between the trends and demographic variables. While the
trends are significant and match the demographic variables in magnitude, the
low $r^2$ of 0.203 for the trends still provides evidence that Google Search
Trends data cannot replace survey analysis when trying to measure rare mask
usage.

\input{tables/paper1/mask_analysis}

\subsection{Validating Google Trends as metrics of health outcomes}

In addition to attitudinal measures, I attempt to validate Google Trends for
uses in the measurements of health indicators. The first indicator I test is
Covid-19 case rates from 2020 through 2021. The Pearson correlation results
indicate negligible to weak relationships between the search terms and the
actual case rates across time and place. The repeated measures correlation
coefficient, a more reliable and less biased measure for longitudinal
correlations, reveals similarly weak results with a maximum correlation of 0.32
between 'smell loss' and rates of Covid-19. To further test the relationship, I
include the three trends in random intercept hierarchical linear models. Model 1
of table \ref{tab:covid_analysis} indicates that the trends themselves are able
to explain about 20\% of the variation in Covid-19 rates ($r^2$ = 0.201). Few
of the demographic variables have an effect on Covid-19 rates in models 2 or 3,
though it seems there is some suppression for median income, unemployment rates,
and population density. Model 3 has an $r^2$ of 0.200. I conclude that the
Google Trends are slightly related to Covid-19 rates, but that this relationship
is weak and we should not attempt to use Google Trends as an indicator of actual
health outcomes.

\input{tables/paper1/covid_analysis}

Another indicator of health outcomes I test to validate Google Trends as
possible health indicators are county suicide rates. As with previous cases,
there is only negligible correlation between actual suicide rates and Google
Search Trends, not exceeding 0.0845. In the more accurate repeated measures
correlation coefficient we see a maximum of 0.142 correlation, remaining
negligible. I introduce these trends into a random intercept model to control
for inter-group variation over time in table \ref{tab:suicide_analysis}. While
table \ref{tab:suicide_analysis} model 1 reveals a significant relationship
between suicide hotline trends and suicide rates, the effect is small and the
entire model only has an $r^2$ of 0.058. Adding in demographic features in model
2 improves the $r^2$ quite a lot. In model 3, the inclusion of Google Trends
actually decreases the overall amount of suicide rate variance explained
compared to model 2. These analyses lead me to conclude that Google Search
Trends should not be used as indicators of suicide rates or intentions.

\input{tables/paper1/suicide_analysis}

\subsection{Validating Google Trends as metrics of political support}

Previous research has showed some relationship between Google Search Trends and
political election results. I test here the relationships between search trends
and U.S. Presidential Election results in 2016 and 2020. In 2016, Pearson
correlations for Both Hilary Clinton and Donald Trump do not exceed a |0.17|
correlation with the actual percentage of votes for either candidate. In 2020,
results are even less related, with searches for neither Joe Biden nor Donald
Trump holding any real correlation with the 2020 results.

In table \ref{tab:pres_2016_analysis}, I outline the results for the model
predicting the county percentage of votes for Hillary Clinton. Model 1
demonstrates how well Google Trends are able to predict the votes; while the
trend for 'Hillary Clinton' is significantly associated with votes, the model's
$r^2$ ($r^2$ = 0.027) shows that it does little to help explain the model
variance. On the other hand, adding in demographic features improves the model
fit quite well, bringing the $r^2$ up to 0.313 in model 2 and $r^2$ 0.327 in
model 3. In table \ref{tab:pres_2020_analysis} shows this same analysis for the
2020 election and predicts the percentage of votes for Joe Biden. While this
model is largely similar to those in table \ref{tab:pres_2016_analysis}, this
model 1 demonstrates how unrelated Google Trends can be from actual outcomes. In
conclusion, I find that Google Trends are an unreliable and invalid indicator of
political support.

\input{tables/paper1/pres_2016_analysis}
\input{tables/paper1/pres_2020_analysis}

\section{Discussion and Conclusion}
This paper provides a critical test of Google Trends for use in Social Science. 
I failed to find moderate or strong correlation among any of the Google 
Trends tested and their validated indicators.
While some Google Trends tested were significantly associated with the outcome in the linear regression model,
effects tended to be small and the total model explainability remained low,
even when controlling for demographic variables.  

Some previous research has shown strong innovation in the computational social sciences
to gather sociodemographic data when survey or other research methods are too costly
\citep{blumenstockPredictingPovertyWealth2015}. Google Trends, in theory, sounds like 
a promising data source to 'nowcast' attitidinal, health, and political indicators;
not only would the data be easily and freely accessible, it would also provide 
indicators for areas where conducting research may be too costly. The demand for this 
sort of indicator is clear and fields like the health sciences have freely utilized
this data source. However, this article shows that there is no 
no external validity of Google Trends for this specific use and social scientists
will find no replacement for high quality survey data with Google Trends. 

As social scientists it is our responsibility to deeply investigate the validity 
of the data that we use in order to be confident in our research findings. As with any
source of data, we must take into account the potential pitfalls, especially in research on big data
\citep{mcfarlandBigDataDanger2015}. In the case of Google Search Trends, precaution was 
warranted. While I would like to be able to join \citet{bailCulturalEnvironmentMeasuring2014} 
in encouraging social scientists to pursue more research with the many new sources of big data, 
I emphasize that testing out data to verify what it actually corrresponds to is an essential step 
to making impactful research as a discipline going forward.  